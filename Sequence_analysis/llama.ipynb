{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef7b86b-70dc-4dc1-bcb9-492e2f8fe694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaModel, AutoModelForCausalLM, LlamaForCausalLM, GenerationConfig, LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489de039-4a87-446e-b114-59faaad93bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局加载LLaMA-2-7B模型\n",
    "model_name_or_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/Llama-2-7b-hf\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer: LlamaTokenizer = LlamaTokenizer.from_pretrained(model_name_or_path, local_files_only=True, model_max_length=512)\n",
    "# 设置pad_token为eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(model_name_or_path, local_files_only=True)\n",
    "model.to(device)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32c6d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(\"introduce the film Marriage story\", return_tensors=\"pt\")\n",
    "output = model.generate(input[\"input_ids\"].cuda(), max_length=512).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ed51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(output[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20abf795-81bc-45f8-bd1c-b761940d255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    return pd.read_pickle(file_path)\n",
    "\n",
    "def load_movie_dict(item_file):\n",
    "    item_df = pd.read_csv(item_file, sep='|', header=None, encoding='latin-1', usecols=[0, 1])\n",
    "    item_df.columns = ['movie_id', 'movie_title']\n",
    "    movie_dict = dict(zip(item_df['movie_id'], item_df['movie_title']))\n",
    "    return movie_dict\n",
    "\n",
    "def map_movie_names_only(seq, movie_dict):\n",
    "    return [movie_dict[id] if id in movie_dict else id for (id, rating) in seq]\n",
    "\n",
    "def extract_sequences(df, movie_dict):\n",
    "    df['movie_names_only'] = df['seq'].apply(lambda x: map_movie_names_only(x, movie_dict))\n",
    "    df['seq_only'] = df['seq'].apply(lambda x: [id for (id, rating) in x])\n",
    "    return df\n",
    "\n",
    "def get_movie_embeddings(movie_list):\n",
    "    embeddings = []\n",
    "    max_length = 512  # 设定一个合理的最大长度\n",
    "    for movies in movie_list:\n",
    "        movie_string = \" \".join(str(movie) for movie in movies)\n",
    "        inputs = tokenizer(movie_string, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            movie_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        embeddings.append(movie_embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def calculate_dtw_distance(embedding_seq1, embedding_seq2):\n",
    "    # 将向量调整为二维数组，以便 fastdtw 正确处理\n",
    "    embedding_seq1 = embedding_seq1.reshape(-1, 1)\n",
    "    embedding_seq2 = embedding_seq2.reshape(-1, 1)\n",
    "    distance, path = fastdtw(embedding_seq1, embedding_seq2, dist=euclidean)\n",
    "    return distance\n",
    "\n",
    "def calculate_similarity(df):\n",
    "    movie_embeddings = get_movie_embeddings(df['movie_names_only'].tolist())\n",
    "    df['movie_embeddings'] = list(movie_embeddings)\n",
    "    embeddings = np.stack(df['movie_embeddings'].values)\n",
    "    \n",
    "    most_similar_indices = []\n",
    "    for i, embedding_seq1 in enumerate(embeddings):\n",
    "        min_distance = float('inf')\n",
    "        most_similar_index = -1\n",
    "        for j, embedding_seq2 in enumerate(embeddings):\n",
    "            if i != j:\n",
    "                distance = calculate_dtw_distance(embedding_seq1, embedding_seq2)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    most_similar_index = j\n",
    "        most_similar_indices.append(most_similar_index)\n",
    "    \n",
    "    df['most_similar_seq_index'] = most_similar_indices\n",
    "    df['most_similar_seq'] = df['most_similar_seq_index'].apply(lambda idx: df.at[idx, 'seq'])\n",
    "    return df\n",
    "\n",
    "def add_most_similar_seq_next(df, movie_dict):\n",
    "    df['most_similar_seq_next'] = df['next'].iloc[df['most_similar_seq_index']].values\n",
    "    df['most_similar_seq_name'] = df['most_similar_seq'].apply(lambda x: [movie_dict.get(item[0], \"Unknown\") for item in x])\n",
    "    df['most_similar_seq_next_name'] = df['most_similar_seq_next'].apply(lambda x: movie_dict.get(x[0], \"Unknown\"))\n",
    "    return df\n",
    "\n",
    "def save_data(df, output_file_path):\n",
    "    df.to_pickle(output_file_path)\n",
    "\n",
    "def process_data(file_path, item_file, output_file_path):\n",
    "    df = load_data(file_path)\n",
    "    movie_dict = load_movie_dict(item_file)\n",
    "    df = extract_sequences(df, movie_dict)\n",
    "    df = calculate_similarity(df)\n",
    "    df = add_most_similar_seq_next(df, movie_dict)\n",
    "    save_data(df, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288a939d-907b-4ead-81a3-7d57b9a54b1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m item_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/LLaRA/data/ref/movielens/u.item\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/LLaRA/data/ref/movielens/similar_val_data.df\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(file_path, item_file, output_file_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_data\u001b[39m(file_path, item_file, output_file_path):\n\u001b[0;32m---> 69\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     movie_dict \u001b[38;5;241m=\u001b[39m load_movie_dict(item_file)\n\u001b[1;32m     71\u001b[0m     df \u001b[38;5;241m=\u001b[39m extract_sequences(df, movie_dict)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 使用函数处理数据\n",
    "file_path = '/workspace/LLaRA/data/ref/movielens/train_data.df'\n",
    "item_file = '/workspace/LLaRA/data/ref/movielens/u.item'\n",
    "output_file_path = '/workspace/LLaRA/data/ref/movielens/similar_val_data.df'\n",
    "\n",
    "process_data(file_path, item_file, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f35e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
